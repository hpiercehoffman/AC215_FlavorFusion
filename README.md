AC215: Milestone 3
==============================

Project Organization
------------
      ├── LICENSE
      ├── README.md
      ├── notebooks
      ├── references
      ├── reports
      │   └── milestone2.md
      ├── requirements.txt
      └── src
            ├── docker-volumes
            │   ├── google-data.dvc
            │   ├── lsars-data.dvc
            │   └── notebooks
            │       ├── process_google.ipynb
            │       ├── process_lsars.ipynb
            │       └── Evaluation_Example.ipynb
            ├── preprocess_google
            │   ├── Dockerfile
            │   ├── Pipfile
            │   ├── Pipfile.lock
            │   ├── preprocess.py
            │   ├── cors.py
            │   ├── utils.py
            │   ├── docker-compose.yml
            │   └── docker-shell.sh	
            ├── preprocess_lsars
            │   ├── Dockerfile
            │   ├── Pipfile
            │   ├── Pipfile.lock
            │   ├── preprocess_lsars.py
            │   ├── docker-compose.yml
            │   └── docker-shell.sh
            └── train
                ├── Dockerfile
                ├── Pipfile
                ├── Pipfile.lock
                ├── train.py
                ├── docker-compose.yml
                └── docker-shell.sh

--------
# Milestone 3 Overview

**Team Members**   
Varun Ullanat, Hannah Pierce-Hoffman

**Group Name**   
FlavorFusion

**Project**   
In this project, we aim to build an app that captures cultural differences in Google resturaunt reviews using abstractive summaries generated by a large language model<sup>1</sup>. 

## Milestone 3 Contents ##

The focus of this milestone is model training. For information on data preprocessing, see our [Milestone 2 report](https://github.com/hpiercehoffman/AC215_FlavorFusion/blob/milestone3/reports/milestone2.md). Data preprocessing takes place in the **preprocess_google** and **preprocess_lsars** Docker containers.

### train ###
This container is responsible for training the [PRIMERA model](https://github.com/allenai/PRIMER) on the [LSARS dataset](https://github.com/ScarletPan/LSARS). 

The inputs to this container are the following:
- 5,000 preprocessed and translated LSARS data points which we generated for Milestone 2, each of which contains:   
    - A unique identifier   
    - 10-40 product reviews   
    - One summary review which captures information from all of the reviews
- Training parameters
- Secrets file (contains service account credentials for Google Cloud)

The output of this container is a file (`pytorch_model.bin`) containing trained model weights. The weight file for each training run is automatically uploaded to Weights and Biases (WandB) at the end of the run.

This container holds the following files:
1. `src/train/train.py` This script trains the PRIMERA model on the LSARS dataset. The script provides options for data augmentation, data streaming, and data downloading from our GCP bucket, as well as standard training parameters such as learning rate and batch size. WandB tracking is integrated into the script, including model upload to WandB at the end of a training run. In our initial runs shown in this milestone, we use pre-trained weights from a version of PRIMERA trained on the [Multi-News dataset](https://huggingface.co/datasets/multi_news). This allows us to leverage transfer learning. In later training runs, we will experiment with using pre-trained weights for only some layers of the model. The PRIMERA model is [hosted](https://huggingface.co/allenai/PRIMERA/tree/main) on Huggingface.
2. `src/train/Dockerfile` This docker sets up a `pipenv` virtual environment for model training. In the file, we use our service account to connect to Google Cloud. We then create a user named `app` and install the required python packages from `src/preprocess_google/Pipfile`. Key packages used in model training include [Pytorch](https://pytorch.org/), [Transformers](https://huggingface.co/docs/transformers/index), and [NLTK](https://www.nltk.org/).
3. `src/train/docker-shell.sh` This script sets up a network so the Docker container can communicate with the outside world via ports. Next, the script builds our Docker container and uses `src/train/docker-compose.yml` to set up the full container environment. In `src/train/docker-compose.yml`, we specify GPU capabilities so the Docker container can take advantage of running on a VM with a GPU.

We ran this container inside a GCP VM with a Nvidia L4 GPU. Due to the large model size, an L4 GPU with 24GB of GPU RAM is required for training. We provide detailed instructions for VM setup and model training in the [Setup Notes](https://github.com/hpiercehoffman/AC215_FlavorFusion/edit/milestone3/README.md#setup-notes) section below.

### docker-volumes ###
This directory is mounted to the `train` Docker container in `src/train/docker-compose.yml`. The `docker-volumes/training-data` directory appears inside the Docker container as `/app/data`. Users may specify this directory as a data download and/or model output directory via parameters to the `train.py` script. This permits the Docker container to access the filesystem of the host VM, allowing downloaded data files and model result files to be saved on the VM even after the Docker container is no longe running.

### notebooks ###    
This directory is currently empty, but will be used to store code which doesn't belong to a specific container, such as reports, insights, or visualizations.

### reports ###
This directory contains our reports from past milestones:
- [Milestone 2](https://github.com/hpiercehoffman/AC215_FlavorFusion/blob/milestone3/reports/milestone2.md): Data preprocessing, Label Studio, and DVC.

### references ###  
(add info here)

## Experiment Tracking ##
Our Weights and Biases page contains two completed training runs, each lasting 10-12 hours. The PRIMERA model has a long training time when run on a single GPU, so each of these runs only covers a few epochs. We will use longer training runs to generate a final set of model weights for the LSARS dataset.

Below we show a view of the WandB dashboard for evaluation metrics. Our most relevant evaluation metric is the (ROUGE score)[https://clementbm.github.io/theory/2021/12/23/rouge-bleu-scores.html#rouge], a NLP metric which measures similarity between the input reviews and the generated summary.

![Evaluation metrics](https://github.com/hpiercehoffman/AC215_FlavorFusion/blob/milestone3/images/eval_metrics.png)

We also show the loss curves on the training set. We use WandB's plotting features to apply mild smoothing, making it easier to observe the overall trend of the loss curves.

![Training loss curves](https://github.com/hpiercehoffman/AC215_FlavorFusion/blob/milestone3/images/train_loss_smoothed.png)

## Serverless Training ##


## Data Pipeline ##
We built our data pipeline in Milestone 2. Our data pipeline uses DVC and Label Studio, as well as preprocessing scripts, to convert data from the **LSARS** and **Google Reviews** datasets into a convenient format for model training. For more information on our data pipeline, including examples showing how to download specific data versions, please see the [Setup Notes](https://github.com/hpiercehoffman/AC215_FlavorFusion/blob/milestone3/reports/milestone2.md#setup-notes) section of our Milestone 2 report.

--------
# Setup Notes #

### GCP VM setup ###

### Running the training Docker container ###   
The Dockerfile for model training can be run via the `docker-shell.sh` script inside the container. To run this container, do the following:
- Clone the repository and checkout the `milestone3` branch
- `cd src/train`
- `chmod 777 docker-shell.sh`
- `./docker-shell.sh`
The Dockerfile for model training will build and you will be dropped into a shell prompt as `app` user. From there, you can kick off a model training run using the `train.py` script.

### Running train.py script ###
To run this script, you must activate the `preprocess_lsars` docker container. Once you are inside the container, you can run the script with the following arguments:


- `-d` or `--download`: Flag to indicate that the untranslated data should be downloaded from our GCS bucket (will not overwrite any previously translated data)
- `--reviews_file_path`: Path to untranslated review file (either pre-mounted or downloaded with the `-d` flag)
- `--start_line`: Line in the input file where translation should start (each line is a single JSON record containing a summary and a group of reviews)
- `--stop_line`: Line in the input file where translation should stop
- `--output_file_path`: Path where translated data will be placed (should be in a mounted volume to avoid losing the data)
- `--upload`: Flag to indicate whether the output file should be uploaded to our GCS bucket 
   
Since the LSARS dataset contains large files, we provide `start_line` and `stop_line` arguments so the user can avoid reading an entire file into memory at once. We also provide the option of downloading the raw data, which is useful if the user is working in a GCP VM without a mounted bucket.     
    
Translating 500 reviews takes about 6 minutes. The script will output a progress bar showing how many records have been translated.


# References #

1. Xiao, Wen, Iz Beltagy, Giuseppe Carenini, and Arman Cohan. “PRIMERA: Pyramid-Based Masked Sentence Pre-Training for Multi-Document Summarization.” arXiv, March 16, 2022. http://arxiv.org/abs/2110.08499.
2. Yan, An, Zhankui He, Jiacheng Li, Tianyang Zhang, and Julian McAuley. “Personalized Showcases: Generating Multi-Modal Explanations for Recommendations.” arXiv, April 6, 2023. http://arxiv.org/abs/2207.00422.
3. Pan, Haojie, Rongqin Yang, Xin Zhou, Rui Wang, Deng Cai, and Xiaozhong Liu. “Large Scale Abstractive Multi-Review Summarization (LSARS) via Aspect Alignment.” In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, 2337–46. Virtual Event China: ACM, 2020. https://doi.org/10.1145/3397271.3401439.
4. Text generated by ChatGPT, OpenAI, March 7, 2023, https://chat.openai.com/chat.



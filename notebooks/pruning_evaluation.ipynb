{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6baf39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "from datasets import load_dataset, load_metric\n",
    "import torch\n",
    "import re\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import os\n",
    "import wandb\n",
    "import random\n",
    "import time\n",
    "\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "import tempfile\n",
    "import zipfile\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    HfArgumentParser,\n",
    "    PreTrainedTokenizer,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dec4455",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_key = \"\"\n",
    "wandb.login(key=wandb_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af707c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    return model, tokenizer\n",
    "\n",
    "def get_wandb_model(path):\n",
    "    run = wandb.init()\n",
    "    artifact = run.use_artifact(path, type=\"model\")\n",
    "    artifact_dir = artifact.download()\n",
    "    return artifact_dir\n",
    "\n",
    "def count_parameters(model): \n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    non_trainable = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "    return trainable, non_trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a355c4",
   "metadata": {},
   "source": [
    "# Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0310f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = get_wandb_model('flavorfusion-team/FlavorFusion/model-w10g07vv:v0')\n",
    "base_model, base_tokenizer = get_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935cb87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = get_wandb_model('flavorfusion-team/FlavorFusion/pruned_model:v0')\n",
    "# pruned_model, pruned_tokenizer = get_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e6aec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_docs(text: str, doc_sep_token: str):\n",
    "    \"\"\"Split a string into multiple reviews based on separator token.\"\"\"\n",
    "    text = re.sub(rf\"{doc_sep_token}$\", \"\", text.strip())\n",
    "    return [doc.strip() for doc in text.split(doc_sep_token)]\n",
    "\n",
    "def get_num_docs(text: str, doc_sep_token: str) -> int:\n",
    "    \"\"\"Get number of reviews in a string with separator token.\"\"\"\n",
    "    return len(list(filter(bool, split_docs(text, doc_sep_token=doc_sep_token))))\n",
    "\n",
    "def sample_reviews(examples, max_docs_per_review=5, k_top_longest=20):\n",
    "    \"\"\"Perform data augmentation by sampling the k longest reviews in a given data point,\n",
    "    then dividing into max_docs number of new data points.\n",
    "    \"\"\"\n",
    "    text_column = 'review_str'\n",
    "    summary_column = 'summary'\n",
    "    \n",
    "    new_reviews = []\n",
    "    new_summaries = []\n",
    "    for i in range(len(examples[text_column])):\n",
    "        summary = examples[summary_column][i]\n",
    "        docs = examples[text_column][i]\n",
    "        docs = split_docs(docs, '|||||')\n",
    "        longest_docs = sorted(docs, key=len, reverse=True)[:k_top_longest]\n",
    "        random.shuffle(longest_docs)\n",
    "        new_docs = [longest_docs[i:i + max_docs_per_review] for i in range(0, len(longest_docs), max_docs_per_review)]\n",
    "        new_docs = ['|||||'.join(new_docs_i) for new_docs_i in new_docs]\n",
    "        new_reviews += new_docs\n",
    "        new_summaries += [summary]*len(new_docs)\n",
    "    return {'augmented_review_str': new_reviews, 'new_summary': new_summaries}\n",
    "\n",
    "\n",
    "def process_document(documents, doc_sep, max_source_length, tokenizer, DOCSEP_TOKEN_ID, PAD_TOKEN_ID):\n",
    "    input_ids_all=[]\n",
    "    for data in documents:\n",
    "        all_docs = data.split(doc_sep)[:-1]\n",
    "        for i, doc in enumerate(all_docs):\n",
    "            doc = doc.replace(\"\\n\", \" \")\n",
    "            doc = \" \".join(doc.split())\n",
    "            all_docs[i] = doc\n",
    "\n",
    "        #### concat with global attention on doc-sep\n",
    "        input_ids = []\n",
    "        for doc in all_docs:\n",
    "            input_ids.extend(\n",
    "                tokenizer.encode(\n",
    "                    doc,\n",
    "                    truncation=True,\n",
    "                    max_length=max_source_length // len(all_docs),\n",
    "                )[1:-1]\n",
    "            )\n",
    "            input_ids.append(DOCSEP_TOKEN_ID)\n",
    "        input_ids = (\n",
    "            [tokenizer.bos_token_id]\n",
    "            + input_ids\n",
    "            + [tokenizer.eos_token_id]\n",
    "        )\n",
    "        input_ids_all.append(torch.tensor(input_ids))\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        input_ids_all, batch_first=True, padding_value=PAD_TOKEN_ID\n",
    "    )\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "def preprocess_function(examples, tokenizer, text_column, summary_column, max_source_length, max_target_length, \n",
    "                        padding=\"max_length\", ignore_pad_token_for_loss=True, prefix=\"\"):\n",
    "    \n",
    "    model_inputs = {}\n",
    "    PAD_TOKEN_ID = tokenizer.pad_token_id\n",
    "    DOCSEP_TOKEN_ID = tokenizer.convert_tokens_to_ids(\"<doc-sep>\")\n",
    "    \n",
    "    inputs, targets = [], []\n",
    "    for i in range(len(examples[text_column])):\n",
    "        if examples[text_column][i] and examples[summary_column][i]:\n",
    "            inputs.append(examples[text_column][i])\n",
    "            targets.append(examples[summary_column][i])\n",
    "\n",
    "    inputs = [prefix + inp for inp in inputs]\n",
    "    \n",
    "    model_inputs['input_ids'] = process_document(inputs,\n",
    "                                                 doc_sep='|||||', \n",
    "                                                 max_source_length=max_source_length,\n",
    "                                                 tokenizer=tokenizer, \n",
    "                                                 DOCSEP_TOKEN_ID=DOCSEP_TOKEN_ID, \n",
    "                                                 PAD_TOKEN_ID=PAD_TOKEN_ID)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\" and ignore_pad_token_for_loss:\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    global_attention_mask = torch.zeros_like(model_inputs['input_ids']).to(model_inputs['input_ids'])\n",
    "    \n",
    "    # put global attention on <s> token\n",
    "    global_attention_mask[:, 0] = 1\n",
    "    global_attention_mask[model_inputs['input_ids'] == DOCSEP_TOKEN_ID] = 1\n",
    "    \n",
    "    model_inputs[\"global_attention_mask\"] = global_attention_mask\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    \"\"\"Decode predictions after forward pass so we can do evaluation metrics.\"\"\"\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42777fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {'train': '*.csv'}\n",
    "raw_dataset = load_dataset('csv', data_files=data_files, split='train', streaming = False)\n",
    "column_names = raw_dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a0332e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = raw_dataset.shuffle(seed=42)\n",
    "raw_dataset = raw_dataset.train_test_split(test_size=0.05)\n",
    "raw_dataset = raw_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44252f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_kwargs = {'max_docs_per_review':5, 'k_top_longest': 5}\n",
    "\n",
    "aug_dataset = raw_dataset.map(\n",
    "            sample_reviews,\n",
    "            fn_kwargs=aug_kwargs,\n",
    "            batched=True,\n",
    "            num_proc=4,\n",
    "            remove_columns=column_names,\n",
    "            desc=\"Augmenting train and test datasets\")\n",
    "\n",
    "token_kwargs = {'text_column': 'augmented_review_str', \n",
    "                'tokenizer': base_tokenizer,\n",
    "                'summary_column': 'new_summary', \n",
    "                'max_source_length': 700, \n",
    "                'max_target_length': 100}\n",
    "\n",
    "full_dataset = aug_dataset.map(\n",
    "            preprocess_function,\n",
    "            fn_kwargs=token_kwargs,\n",
    "            batched=True,\n",
    "            num_proc=4,\n",
    "            desc=\"Running tokenizer on train and test datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c576a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc29e402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_batch(batch, model, tokenizer):\n",
    "    input_ids = batch['input_ids']\n",
    "\n",
    "    # get the input ids and attention masks together\n",
    "    global_attention_mask = batch['global_attention_mask']\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        input_ids=torch.tensor(input_ids).to(model.device),\n",
    "        global_attention_mask=torch.tensor(global_attention_mask).to(model.device),\n",
    "        use_cache=True,\n",
    "        max_length=100,\n",
    "        num_beams=1)\n",
    "    \n",
    "    generated_str = tokenizer.batch_decode(generated_ids.tolist(), skip_special_tokens=True)\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    result={}\n",
    "    result['generated_summaries'] = generated_str\n",
    "    result['gt_summaries']=batch['new_summary']\n",
    "\n",
    "    score = metric.compute(predictions=generated_str, references=batch['new_summary'])\n",
    "    scores = {key: [value.mid.fmeasure * 100] for key, value in score.items()}\n",
    "    \n",
    "    result['rouge1'] = scores['rouge1']\n",
    "    result['rouge2'] = scores['rouge2']\n",
    "    result['rougeL'] = scores['rougeL']\n",
    "    result['rougeLsum'] = scores['rougeLsum']\n",
    "    result['inference_time'] = [end-start]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bb71d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_kwargs = {'model': base_model, \n",
    "                'tokenizer': base_tokenizer}\n",
    "\n",
    "res = full_dataset.map(inference_batch, fn_kwargs=eval_kwargs, batched=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d30dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_metrics(result):\n",
    "    metrics_list = ['rouge1', 'rouge2', 'rougeL', 'rougeLsum', 'inference_time']\n",
    "    avg = [sum(result[met])/len(res) for met in metrics_list]\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e599463",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(average_metrics(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd36a6a",
   "metadata": {},
   "source": [
    "# Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864429f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = get_wandb_model('flavorfusion-team/FlavorFusion/pruned_model:v0')\n",
    "pruned_model, pruned_tokenizer = get_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2771dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_kwargs = {'model': pruned_model, \n",
    "                'tokenizer': pruned_tokenizer}\n",
    "\n",
    "res = full_dataset.take(5).map(inference_batch, fn_kwargs=eval_kwargs, batched=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd0935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(average_metrics(res))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
